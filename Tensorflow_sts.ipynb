{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "data_path=SICK\n",
      "embedding_dim=300\n",
      "max_length=26\n",
      "save_path=SICK/STS_log\n",
      "use_fp64=False\n",
      "word2vec_norm=embeddings/word2vec_norm.txt\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_norm','embeddings/word2vec_norm.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','SICK','SICK data set path')\n",
    "flags.DEFINE_string('save_path','SICK/STS_log','STS model output directory')\n",
    "flags.DEFINE_integer('embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('max_length',26,'one sentence max length words which is in dictionary')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path=None):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            vocab_size,layer2_size=map(int,header.split())\n",
    "            # initial matrix with random uniform\n",
    "            init_W=np.random.uniform(-0.25,0.25,(vocab_size,FLAGS.embedding_dim))\n",
    "\n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            dictionary=dict()\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                dictionary[word]=len(dictionary)\n",
    "                init_W[dictionary[word]]=np.array(line.split()[1:], dtype=np.float32)\n",
    "\n",
    "        return dictionary,init_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename,word_to_id,is_test=False):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        pairIDs=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            ID=line.split('\\t')[0] # for test\n",
    "            pairIDs.append(ID)\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            _=[word_to_id[word] for word in sentence_A.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_A.append(_)\n",
    "            sentencesA_length.append(len(_))\n",
    "            _=[word_to_id[word] for word in sentence_B.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_B.append(_)\n",
    "            sentencesB_length.append(len(_))\n",
    "            relatedness_scores.append((float(relatedness_score)-1)/4)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    if not is_test: return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "    else:\n",
    "        stsinput=STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "        stsinput.pairIDs=pairIDs\n",
    "        return stsinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSInput(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec_norm file embeddings/word2vec_norm.txt\n",
      "vocab_size=2378\n"
     ]
    }
   ],
   "source": [
    "train_path=os.path.join(FLAGS.data_path,'SICK_new_train.txt')\n",
    "valid_path=os.path.join(FLAGS.data_path,'SICK_new_trial.txt')\n",
    "test_path=os.path.join(FLAGS.data_path,'SICK_test_annotated.txt')\n",
    "\n",
    "dictionary,init_W=build_vocab(FLAGS.word2vec_norm)\n",
    "train_data=file_to_word_ids(train_path,dictionary)\n",
    "valid_data=file_to_word_ids(valid_path,dictionary)\n",
    "test_data=file_to_word_ids(test_path,dictionary,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(start,end,input):\n",
    "    inputs_A=input.sentences_A[start:end]\n",
    "    inputsA_length=input.sentencesA_length[start:end]\n",
    "    inputs_B=input.sentences_B[start:end]\n",
    "    inputsB_length=input.sentencesB_length[start:end]\n",
    "    labels=np.reshape(input.relatedness_scores[start:end],(len(range(start,end)),1))\n",
    "    return STSInput(inputs_A,inputsA_length,inputs_B,inputsB_length,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    init_scale=0.04\n",
    "    learning_rate=3.\n",
    "    max_grad_norm=10\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.98\n",
    "    batch_size=20\n",
    "    max_epoch=14\n",
    "    max_max_epoch=130\n",
    "    \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_,input_length,dropout_):\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(num_units=50,forget_bias=0.0)\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell])\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch size: 175\n",
      "Epoch 0 Learning rate: 3.0\n",
      "Average cost:\t0.0719449244227\n",
      "Valid cost:\t0.205882653594\n",
      "Epoch 1 Learning rate: 3.0\n",
      "Average cost:\t0.0676207906646\n",
      "Valid cost:\t0.205936163664\n",
      "Epoch 2 Learning rate: 3.0\n",
      "Average cost:\t0.0675960755348\n",
      "Valid cost:\t0.205971643329\n",
      "Epoch 3 Learning rate: 3.0\n",
      "Average cost:\t0.0675394051096\n",
      "Valid cost:\t0.205998048186\n",
      "Epoch 4 Learning rate: 3.0\n",
      "Average cost:\t0.0674916012692\n",
      "Valid cost:\t0.20601528883\n",
      "Epoch 5 Learning rate: 3.0\n",
      "Average cost:\t0.0676040165871\n",
      "Valid cost:\t0.206030651927\n",
      "Epoch 6 Learning rate: 3.0\n",
      "Average cost:\t0.0674058251509\n",
      "Valid cost:\t0.206044599414\n",
      "Epoch 7 Learning rate: 3.0\n",
      "Average cost:\t0.0674860215826\n",
      "Valid cost:\t0.206054612994\n",
      "Epoch 8 Learning rate: 3.0\n",
      "Average cost:\t0.0676661910649\n",
      "Valid cost:\t0.206062093377\n",
      "Epoch 9 Learning rate: 3.0\n",
      "Average cost:\t0.0674298331354\n",
      "Valid cost:\t0.206070125103\n",
      "Epoch 10 Learning rate: 3.0\n",
      "Average cost:\t0.0675701573598\n",
      "Valid cost:\t0.206076055765\n",
      "Epoch 11 Learning rate: 3.0\n",
      "Average cost:\t0.0674864529605\n",
      "Valid cost:\t0.206081569195\n",
      "Epoch 12 Learning rate: 3.0\n",
      "Average cost:\t0.0674752563877\n",
      "Valid cost:\t0.206087112427\n",
      "Epoch 13 Learning rate: 3.0\n",
      "Average cost:\t0.0673434147771\n",
      "Valid cost:\t0.206089660525\n",
      "Epoch 14 Learning rate: 2.94000005722\n",
      "Average cost:\t0.0674340545173\n",
      "Valid cost:\t0.206093207002\n",
      "Epoch 15 Learning rate: 2.88120007515\n",
      "Average cost:\t0.0676339341487\n",
      "Valid cost:\t0.206098079681\n",
      "Epoch 16 Learning rate: 2.82357597351\n",
      "Average cost:\t0.0675310856955\n",
      "Valid cost:\t0.206101164222\n",
      "Epoch 17 Learning rate: 2.76710438728\n",
      "Average cost:\t0.067644930599\n",
      "Valid cost:\t0.206103429198\n",
      "Epoch 18 Learning rate: 2.71176242828\n",
      "Average cost:\t0.0674680694938\n",
      "Valid cost:\t0.206106111407\n",
      "Epoch 19 Learning rate: 2.65752720833\n",
      "Average cost:\t0.0674026687763\n",
      "Valid cost:\t0.20610819757\n",
      "Epoch 20 Learning rate: 2.60437655449\n",
      "Average cost:\t0.0675558965227\n",
      "Valid cost:\t0.206109941006\n",
      "Epoch 21 Learning rate: 2.55228900909\n",
      "Average cost:\t0.0674228642562\n",
      "Valid cost:\t0.206111848354\n",
      "Epoch 22 Learning rate: 2.50124335289\n",
      "Average cost:\t0.0673391507034\n",
      "Valid cost:\t0.206112712622\n",
      "Epoch 23 Learning rate: 2.45121836662\n",
      "Average cost:\t0.0675374913961\n",
      "Valid cost:\t0.206114143133\n",
      "Epoch 24 Learning rate: 2.40219402313\n",
      "Average cost:\t0.0674530081345\n",
      "Valid cost:\t0.206115186214\n",
      "Epoch 25 Learning rate: 2.35415005684\n",
      "Average cost:\t0.0676120545928\n",
      "Valid cost:\t0.206116169691\n",
      "Epoch 26 Learning rate: 2.30706715584\n",
      "Average cost:\t0.0675818699918\n",
      "Valid cost:\t0.206117749214\n",
      "Epoch 27 Learning rate: 2.26092576981\n",
      "Average cost:\t0.0674391030733\n",
      "Valid cost:\t0.206118643284\n",
      "Epoch 28 Learning rate: 2.21570730209\n",
      "Average cost:\t0.0673977043586\n",
      "Valid cost:\t0.206119567156\n",
      "Epoch 29 Learning rate: 2.17139315605\n",
      "Average cost:\t0.067538391043\n",
      "Valid cost:\t0.206120193005\n",
      "Epoch 30 Learning rate: 2.12796521187\n",
      "Average cost:\t0.0673270132499\n",
      "Valid cost:\t0.206121593714\n",
      "Epoch 31 Learning rate: 2.08540606499\n",
      "Average cost:\t0.0674928478471\n",
      "Valid cost:\t0.206121891737\n",
      "Epoch 32 Learning rate: 2.04369783401\n",
      "Average cost:\t0.0676214550861\n",
      "Valid cost:\t0.206122308969\n",
      "Epoch 33 Learning rate: 2.00282382965\n",
      "Average cost:\t0.0674520141418\n",
      "Valid cost:\t0.20612333715\n",
      "Epoch 34 Learning rate: 1.9627674818\n",
      "Average cost:\t0.0674480031005\n",
      "Valid cost:\t0.206123530865\n",
      "Epoch 35 Learning rate: 1.92351210117\n",
      "Average cost:\t0.0675401606091\n",
      "Valid cost:\t0.20612500608\n",
      "Epoch 36 Learning rate: 1.88504183292\n",
      "Average cost:\t0.0672287638273\n",
      "Valid cost:\t0.20612488687\n",
      "Epoch 37 Learning rate: 1.84734106064\n",
      "Average cost:\t0.067516636444\n",
      "Valid cost:\t0.206125304103\n",
      "Epoch 38 Learning rate: 1.8103941679\n",
      "Average cost:\t0.067565455011\n",
      "Valid cost:\t0.20612564683\n",
      "Epoch 39 Learning rate: 1.77418625355\n",
      "Average cost:\t0.0674843632856\n",
      "Valid cost:\t0.206126451492\n",
      "Epoch 40 Learning rate: 1.73870253563\n",
      "Average cost:\t0.0673836237511\n",
      "Valid cost:\t0.206127330661\n",
      "Epoch 41 Learning rate: 1.70392847061\n",
      "Average cost:\t0.0675672131138\n",
      "Valid cost:\t0.2061278373\n",
      "Epoch 42 Learning rate: 1.6698499918\n",
      "Average cost:\t0.0673705119001\n",
      "Valid cost:\t0.2061278373\n",
      "Epoch 43 Learning rate: 1.63645291328\n",
      "Average cost:\t0.0673213853048\n",
      "Valid cost:\t0.206127747893\n",
      "Epoch 44 Learning rate: 1.60372388363\n",
      "Average cost:\t0.0675130761095\n",
      "Valid cost:\t0.206128135324\n",
      "Epoch 45 Learning rate: 1.57164943218\n",
      "Average cost:\t0.0673777796328\n",
      "Valid cost:\t0.206128448248\n",
      "Epoch 46 Learning rate: 1.54021644592\n",
      "Average cost:\t0.0673384028673\n",
      "Valid cost:\t0.206129193306\n",
      "Epoch 47 Learning rate: 1.50941205025\n",
      "Average cost:\t0.0674825658436\n",
      "Valid cost:\t0.206129223108\n",
      "Epoch 48 Learning rate: 1.47922384739\n",
      "Average cost:\t0.0675438829405\n",
      "Valid cost:\t0.206129312515\n",
      "Epoch 49 Learning rate: 1.44963943958\n",
      "Average cost:\t0.0675364858231\n",
      "Valid cost:\t0.206129699945\n",
      "Epoch 50 Learning rate: 1.42064654827\n",
      "Average cost:\t0.0674086734972\n",
      "Valid cost:\t0.206130102277\n",
      "Epoch 51 Learning rate: 1.39223361015\n",
      "Average cost:\t0.0674829400863\n",
      "Valid cost:\t0.206130340695\n",
      "Epoch 52 Learning rate: 1.36438894272\n",
      "Average cost:\t0.0675219416512\n",
      "Valid cost:\t0.206130459905\n",
      "Epoch 53 Learning rate: 1.33710122108\n",
      "Average cost:\t0.0675470217104\n",
      "Valid cost:\t0.206130340695\n",
      "Epoch 54 Learning rate: 1.31035923958\n",
      "Average cost:\t0.0673724243471\n",
      "Valid cost:\t0.206130817533\n",
      "Epoch 55 Learning rate: 1.28415203094\n",
      "Average cost:\t0.067260104801\n",
      "Valid cost:\t0.206131011248\n",
      "Epoch 56 Learning rate: 1.25846898556\n",
      "Average cost:\t0.0673568131136\n",
      "Valid cost:\t0.206131234765\n",
      "Epoch 57 Learning rate: 1.233299613\n",
      "Average cost:\t0.0675227291776\n",
      "Valid cost:\t0.206131264567\n",
      "Epoch 58 Learning rate: 1.20863354206\n",
      "Average cost:\t0.0674479602916\n",
      "Valid cost:\t0.206131458282\n",
      "Epoch 59 Learning rate: 1.18446087837\n",
      "Average cost:\t0.0675223688781\n",
      "Valid cost:\t0.206131517887\n",
      "Epoch 60 Learning rate: 1.16077172756\n",
      "Average cost:\t0.0672995607661\n",
      "Valid cost:\t0.206131547689\n",
      "Epoch 61 Learning rate: 1.13755631447\n",
      "Average cost:\t0.0672849941147\n",
      "Valid cost:\t0.206131771207\n",
      "Epoch 62 Learning rate: 1.11480510235\n",
      "Average cost:\t0.0673036255581\n",
      "Valid cost:\t0.206131950021\n",
      "Epoch 63 Learning rate: 1.0925090313\n",
      "Average cost:\t0.0671833506546\n",
      "Valid cost:\t0.206131920218\n",
      "Epoch 64 Learning rate: 1.07065880299\n",
      "Average cost:\t0.0672405839286\n",
      "Valid cost:\t0.206132233143\n",
      "Epoch 65 Learning rate: 1.04924571514\n",
      "Average cost:\t0.0674333921288\n",
      "Valid cost:\t0.206132128835\n",
      "Epoch 66 Learning rate: 1.02826082706\n",
      "Average cost:\t0.0673308710009\n",
      "Valid cost:\t0.206132203341\n",
      "Epoch 67 Learning rate: 1.00769555569\n",
      "Average cost:\t0.0673568005647\n",
      "Valid cost:\t0.206132113934\n",
      "Epoch 68 Learning rate: 0.987541615963\n",
      "Average cost:\t0.0674060851016\n",
      "Valid cost:\t0.206132248044\n",
      "Epoch 69 Learning rate: 0.967790782452\n",
      "Average cost:\t0.0674266882986\n",
      "Valid cost:\t0.206132516265\n",
      "Epoch 70 Learning rate: 0.948435008526\n",
      "Average cost:\t0.0675645708825\n",
      "Valid cost:\t0.206132948399\n",
      "Epoch 71 Learning rate: 0.929466307163\n",
      "Average cost:\t0.0675547281759\n",
      "Valid cost:\t0.206132605672\n",
      "Epoch 72 Learning rate: 0.910876989365\n",
      "Average cost:\t0.0675077970858\n",
      "Valid cost:\t0.206132665277\n",
      "Epoch 73 Learning rate: 0.892659425735\n",
      "Average cost:\t0.067452428607\n",
      "Valid cost:\t0.206133335829\n",
      "Epoch 74 Learning rate: 0.8748062253\n",
      "Average cost:\t0.0674251096696\n",
      "Valid cost:\t0.206132933497\n",
      "Epoch 75 Learning rate: 0.857310116291\n",
      "Average cost:\t0.0673083607959\n",
      "Valid cost:\t0.206132903695\n",
      "Epoch 76 Learning rate: 0.840163886547\n",
      "Average cost:\t0.0674937383298\n",
      "Valid cost:\t0.206132829189\n",
      "Epoch 77 Learning rate: 0.823360621929\n",
      "Average cost:\t0.067212337958\n",
      "Valid cost:\t0.206133335829\n",
      "Epoch 78 Learning rate: 0.806893408298\n",
      "Average cost:\t0.0672877072969\n",
      "Valid cost:\t0.2061329633\n",
      "Epoch 79 Learning rate: 0.790755569935\n",
      "Average cost:\t0.0673527270768\n",
      "Valid cost:\t0.20613335073\n",
      "Epoch 80 Learning rate: 0.774940431118\n",
      "Average cost:\t0.0674981023903\n",
      "Valid cost:\t0.206133469939\n",
      "Epoch 81 Learning rate: 0.759441614151\n",
      "Average cost:\t0.0675125467032\n",
      "Valid cost:\t0.206133425236\n",
      "Epoch 82 Learning rate: 0.744252800941\n",
      "Average cost:\t0.0674413736058\n",
      "Valid cost:\t0.206133261323\n",
      "Epoch 83 Learning rate: 0.729367733002\n",
      "Average cost:\t0.0674736824951\n",
      "Valid cost:\t0.206133559346\n",
      "Epoch 84 Learning rate: 0.714780390263\n",
      "Average cost:\t0.0674085808439\n",
      "Valid cost:\t0.206133499742\n",
      "Epoch 85 Learning rate: 0.700484752655\n",
      "Average cost:\t0.0674584408211\n",
      "Valid cost:\t0.206133499742\n",
      "Epoch 86 Learning rate: 0.686475098133\n",
      "Average cost:\t0.0675555366597\n",
      "Valid cost:\t0.206133171916\n",
      "Epoch 87 Learning rate: 0.672745585442\n",
      "Average cost:\t0.0674953587992\n",
      "Valid cost:\t0.206133633852\n",
      "Epoch 88 Learning rate: 0.659290671349\n",
      "Average cost:\t0.0674331055156\n",
      "Valid cost:\t0.206133767962\n",
      "Epoch 89 Learning rate: 0.646104872227\n",
      "Average cost:\t0.0672859392528\n",
      "Valid cost:\t0.206133693457\n",
      "Epoch 90 Learning rate: 0.633182764053\n",
      "Average cost:\t0.0674698980153\n",
      "Valid cost:\t0.206133797765\n",
      "Epoch 91 Learning rate: 0.62051910162\n",
      "Average cost:\t0.0673195025857\n",
      "Valid cost:\t0.206133931875\n",
      "Epoch 92 Learning rate: 0.608108699322\n",
      "Average cost:\t0.0673827582066\n",
      "Valid cost:\t0.206133827567\n",
      "Epoch 93 Learning rate: 0.595946550369\n",
      "Average cost:\t0.0672888061191\n",
      "Valid cost:\t0.206133916974\n",
      "Epoch 94 Learning rate: 0.584027647972\n",
      "Average cost:\t0.0672959916826\n",
      "Valid cost:\t0.206134080887\n",
      "Epoch 95 Learning rate: 0.572347044945\n",
      "Average cost:\t0.0673594731412\n",
      "Valid cost:\t0.20613399148\n",
      "Epoch 96 Learning rate: 0.56090015173\n",
      "Average cost:\t0.0673656638392\n",
      "Valid cost:\t0.206134065986\n",
      "Epoch 97 Learning rate: 0.54968214035\n",
      "Average cost:\t0.0674733309874\n",
      "Valid cost:\t0.206134140491\n",
      "Epoch 98 Learning rate: 0.538688480854\n",
      "Average cost:\t0.0673392837814\n",
      "Valid cost:\t0.206133961678\n",
      "Epoch 99 Learning rate: 0.527914702892\n",
      "Average cost:\t0.0673654125737\n",
      "Valid cost:\t0.20613437891\n",
      "Epoch 100 Learning rate: 0.517356395721\n",
      "Average cost:\t0.0673883185536\n",
      "Valid cost:\t0.206134065986\n",
      "Epoch 101 Learning rate: 0.507009267807\n",
      "Average cost:\t0.0674110775547\n",
      "Valid cost:\t0.206134021282\n",
      "Epoch 102 Learning rate: 0.496869117022\n",
      "Average cost:\t0.0673842300688\n",
      "Valid cost:\t0.206133961678\n",
      "Epoch 103 Learning rate: 0.486931711435\n",
      "Average cost:\t0.0674077950205\n",
      "Valid cost:\t0.206134483218\n",
      "Epoch 104 Learning rate: 0.477193087339\n",
      "Average cost:\t0.0673765803767\n",
      "Valid cost:\t0.206134483218\n",
      "Epoch 105 Learning rate: 0.46764922142\n",
      "Average cost:\t0.0674729884309\n",
      "Valid cost:\t0.206134364009\n",
      "Epoch 106 Learning rate: 0.458296239376\n",
      "Average cost:\t0.0675110932333\n",
      "Valid cost:\t0.206134274602\n",
      "Epoch 107 Learning rate: 0.449130326509\n",
      "Average cost:\t0.067493039029\n",
      "Valid cost:\t0.206133946776\n",
      "Epoch 108 Learning rate: 0.440147697926\n",
      "Average cost:\t0.0673514185314\n",
      "Valid cost:\t0.206134274602\n",
      "Epoch 109 Learning rate: 0.431344747543\n",
      "Average cost:\t0.0673418984136\n",
      "Valid cost:\t0.206133812666\n",
      "Epoch 110 Learning rate: 0.422717869282\n",
      "Average cost:\t0.067366669508\n",
      "Valid cost:\t0.206134006381\n",
      "Epoch 111 Learning rate: 0.414263516665\n",
      "Average cost:\t0.0673282123038\n",
      "Valid cost:\t0.206133976579\n",
      "Epoch 112 Learning rate: 0.405978232622\n",
      "Average cost:\t0.0673570055302\n",
      "Valid cost:\t0.206133946776\n",
      "Epoch 113 Learning rate: 0.397858679295\n",
      "Average cost:\t0.0674027031447\n",
      "Valid cost:\t0.206134110689\n",
      "Epoch 114 Learning rate: 0.389901489019\n",
      "Average cost:\t0.0673346722552\n",
      "Valid cost:\t0.206134170294\n",
      "Epoch 115 Learning rate: 0.382103472948\n",
      "Average cost:\t0.0674185956695\n",
      "Valid cost:\t0.206134423614\n",
      "Epoch 116 Learning rate: 0.374461382627\n",
      "Average cost:\t0.0674526261858\n",
      "Valid cost:\t0.206134170294\n",
      "Epoch 117 Learning rate: 0.366972178221\n",
      "Average cost:\t0.0674421753841\n",
      "Valid cost:\t0.206134021282\n",
      "Epoch 118 Learning rate: 0.359632730484\n",
      "Average cost:\t0.0675628754497\n",
      "Valid cost:\t0.206134214997\n",
      "Epoch 119 Learning rate: 0.352440059185\n",
      "Average cost:\t0.0676352793723\n",
      "Valid cost:\t0.2061342448\n",
      "Epoch 120 Learning rate: 0.345391273499\n",
      "Average cost:\t0.0673040239406\n",
      "Valid cost:\t0.206134289503\n",
      "Epoch 121 Learning rate: 0.338483452797\n",
      "Average cost:\t0.0674986284013\n",
      "Valid cost:\t0.206134498119\n",
      "Epoch 122 Learning rate: 0.33171376586\n",
      "Average cost:\t0.0672943214221\n",
      "Valid cost:\t0.206134438515\n",
      "Epoch 123 Learning rate: 0.325079500675\n",
      "Average cost:\t0.0674910337797\n",
      "Valid cost:\t0.206134259701\n",
      "Epoch 124 Learning rate: 0.31857791543\n",
      "Average cost:\t0.0674130407827\n",
      "Valid cost:\t0.206134259701\n",
      "Epoch 125 Learning rate: 0.312206357718\n",
      "Average cost:\t0.0673571558403\n",
      "Valid cost:\t0.206134364009\n",
      "Epoch 126 Learning rate: 0.305962234735\n",
      "Average cost:\t0.067379096193\n",
      "Valid cost:\t0.206134364009\n",
      "Epoch 127 Learning rate: 0.299842983484\n",
      "Average cost:\t0.0673582035622\n",
      "Valid cost:\t0.206134438515\n",
      "Epoch 128 Learning rate: 0.293846130371\n",
      "Average cost:\t0.0673057948053\n",
      "Valid cost:\t0.206134423614\n",
      "Epoch 129 Learning rate: 0.287969201803\n",
      "Average cost:\t0.067481003574\n",
      "Valid cost:\t0.206134334207\n",
      "0.198618\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None,1]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        last_A=tf.transpose(outputs_A,[1,0,2])[-1]\n",
    "        last_B=tf.transpose(outputs_B,[1,0,2])[-1]\n",
    "        prediction=tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.abs(tf.sub(last_A,last_B)),1)))\n",
    "\n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver = tf.train.Saver()\n",
    "            #new_saver = tf.train.import_meta_graph('SICK/STS_log/sts-model-11250.meta')\n",
    "            #new_saver.restore(sess, 'SICK/STS_log/sts-model-11250')\n",
    "            total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "            print('Total batch size: {}'.format(total_batch))\n",
    "            \n",
    "            # train\n",
    "            for epoch in range(config.max_max_epoch):\n",
    "                lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "                avg_cost=0.\n",
    "                for i in range(total_batch):\n",
    "                    start=i*config.batch_size\n",
    "                    end=(i+1)*config.batch_size\n",
    "\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                    \n",
    "                print('Average cost:\\t{}'.format(avg_cost/total_batch))\n",
    "                \n",
    "                # validation\n",
    "                valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                    sentences_A:valid_data.sentences_A,\n",
    "                    sentencesA_length:valid_data.sentencesA_length,\n",
    "                    sentences_B:valid_data.sentences_B,\n",
    "                    sentencesB_length:valid_data.sentencesB_length,\n",
    "                    labels:np.reshape(valid_data.relatedness_scores,(len(valid_data.relatedness_scores),1)),\n",
    "                    embedding_placeholder:init_W,\n",
    "                    dropout_f:1.0\n",
    "                })\n",
    "                print('Valid cost:\\t{}'.format(valid_cost))\n",
    "                \n",
    "            saver.save(sess, 'SICK/STS_log/sts-model', global_step=config.max_max_epoch)\n",
    "            \n",
    "            # test\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "                sentences_A:test_data.sentences_A,\n",
    "                sentencesA_length:test_data.sentencesA_length,\n",
    "                sentences_B:test_data.sentences_B,\n",
    "                sentencesB_length:test_data.sentencesB_length,\n",
    "                labels:np.reshape(test_data.relatedness_scores,(len(test_data.relatedness_scores),1)),\n",
    "                embedding_placeholder:init_W,\n",
    "                dropout_f:1.0\n",
    "            })\n",
    "            print(test_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
