{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "data_path=SICK\n",
      "embedding_dim=300\n",
      "max_length=26\n",
      "save_path=SICK/STS_log\n",
      "use_fp64=False\n",
      "word2vec_norm=embeddings/word2vec_norm.txt\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_norm','embeddings/word2vec_norm.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','SICK','SICK data set path')\n",
    "flags.DEFINE_string('save_path','SICK/STS_log','STS model output directory')\n",
    "flags.DEFINE_integer('embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('max_length',26,'one sentence max length words which is in dictionary')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path=None):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            vocab_size,layer2_size=map(int,header.split())\n",
    "            # initial matrix with random uniform\n",
    "            init_W=np.random.uniform(-0.25,0.25,(vocab_size,FLAGS.embedding_dim))\n",
    "\n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            dictionary=dict()\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                dictionary[word]=len(dictionary)\n",
    "                init_W[dictionary[word]]=np.array(line.split()[1:], dtype=np.float32)\n",
    "\n",
    "        return dictionary,init_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename,word_to_id,is_test=False):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        pairIDs=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            ID=line.split('\\t')[0] # for test\n",
    "            pairIDs.append(ID)\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            _=[word_to_id[word] for word in sentence_A.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_A.append(_)\n",
    "            sentencesA_length.append(len(_))\n",
    "            _=[word_to_id[word] for word in sentence_B.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_B.append(_)\n",
    "            sentencesB_length.append(len(_))\n",
    "            relatedness_scores.append((float(relatedness_score)-1)/4)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    if not is_test: return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "    else:\n",
    "        stsinput=STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "        stsinput.pairIDs=pairIDs\n",
    "        return stsinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSInput(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec_norm file embeddings/word2vec_norm.txt\n",
      "vocab_size=2378\n"
     ]
    }
   ],
   "source": [
    "train_path=os.path.join(FLAGS.data_path,'SICK_train.txt')\n",
    "valid_path=os.path.join(FLAGS.data_path,'SICK_trial.txt')\n",
    "test_path=os.path.join(FLAGS.data_path,'SICK_test_annotated.txt')\n",
    "\n",
    "dictionary,init_W=build_vocab(FLAGS.word2vec_norm)\n",
    "train_data=file_to_word_ids(train_path,dictionary)\n",
    "valid_data=file_to_word_ids(valid_path,dictionary)\n",
    "test_data=file_to_word_ids(test_path,dictionary,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(start,end,input):\n",
    "    inputs_A=input.sentences_A[start:end]\n",
    "    inputsA_length=input.sentencesA_length[start:end]\n",
    "    inputs_B=input.sentences_B[start:end]\n",
    "    inputsB_length=input.sentencesB_length[start:end]\n",
    "    labels=np.reshape(input.relatedness_scores[start:end],(len(range(start,end)),1))\n",
    "    return STSInput(inputs_A,inputsA_length,inputs_B,inputsB_length,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_,input_length,dropout_):\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(num_units=50,forget_bias=0.0,state_is_tuple=True)\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    #rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*50,state_is_tuple=True)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    init_scale=0.1\n",
    "    learning_rate=10\n",
    "    max_grad_norm=1\n",
    "    keep_prob=1.0\n",
    "    lr_decay=0.9\n",
    "    batch_size=20\n",
    "    max_epoch=15\n",
    "    max_max_epoch=200\n",
    "    \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch size: 225\n",
      "Epoch 0 Learning rate: 10.0\n",
      "Average cost: 0.120876379849\n",
      "Epoch 1 Learning rate: 10.0\n",
      "Average cost: 0.112806319189\n",
      "Epoch 2 Learning rate: 10.0\n",
      "Average cost: 0.130850661132\n",
      "Epoch 3 Learning rate: 10.0\n",
      "Average cost: 0.140222729726\n",
      "Epoch 4 Learning rate: 10.0\n",
      "Average cost: 0.120781012409\n",
      "Epoch 5 Learning rate: 10.0\n",
      "Average cost: 0.120872950604\n",
      "Epoch 6 Learning rate: 10.0\n",
      "Average cost: 0.12807044213\n",
      "Epoch 7 Learning rate: 10.0\n",
      "Average cost: 0.126877213816\n",
      "Epoch 8 Learning rate: 10.0\n",
      "Average cost: 0.132872005105\n",
      "Epoch 9 Learning rate: 10.0\n",
      "Average cost: 0.139560285509\n",
      "Epoch 10 Learning rate: 10.0\n",
      "Average cost: 0.156146437145\n",
      "Epoch 11 Learning rate: 10.0\n",
      "Average cost: 0.137624321298\n",
      "Epoch 12 Learning rate: 10.0\n",
      "Average cost: 0.148955358565\n",
      "Epoch 13 Learning rate: 10.0\n",
      "Average cost: 0.146261130604\n",
      "Epoch 14 Learning rate: 10.0\n",
      "Average cost: 0.144788722611\n",
      "Epoch 15 Learning rate: 9.0\n",
      "Average cost: 0.146619875067\n",
      "Epoch 16 Learning rate: 8.10000038147\n",
      "Average cost: 0.145593833394\n",
      "Epoch 17 Learning rate: 7.28999996185\n",
      "Average cost: 0.141744056708\n",
      "Epoch 18 Learning rate: 6.5609998703\n",
      "Average cost: 0.143280481199\n",
      "Epoch 19 Learning rate: 5.90490007401\n",
      "Average cost: 0.141024120069\n",
      "Epoch 20 Learning rate: 5.31441020966\n",
      "Average cost: 0.150642360846\n",
      "Epoch 21 Learning rate: 4.78296899796\n",
      "Average cost: 0.148442851073\n",
      "Epoch 22 Learning rate: 4.30467224121\n",
      "Average cost: 0.147381061431\n",
      "Epoch 23 Learning rate: 3.87420487404\n",
      "Average cost: 0.147926168988\n",
      "Epoch 24 Learning rate: 3.48678445816\n",
      "Average cost: 0.146796525021\n",
      "Epoch 25 Learning rate: 3.13810586929\n",
      "Average cost: 0.146313803891\n",
      "Epoch 26 Learning rate: 2.82429528236\n",
      "Average cost: 0.148235154069\n",
      "Epoch 27 Learning rate: 2.54186582565\n",
      "Average cost: 0.136124668734\n",
      "Epoch 28 Learning rate: 2.2876791954\n",
      "Average cost: 0.145594464458\n",
      "Epoch 29 Learning rate: 2.05891132355\n",
      "Average cost: 0.142368885378\n",
      "Epoch 30 Learning rate: 1.85302019119\n",
      "Average cost: 0.131113174558\n",
      "Epoch 31 Learning rate: 1.66771817207\n",
      "Average cost: 0.138328137166\n",
      "Epoch 32 Learning rate: 1.50094640255\n",
      "Average cost: 0.154660007523\n",
      "Epoch 33 Learning rate: 1.35085177422\n",
      "Average cost: 0.146215587109\n",
      "Epoch 34 Learning rate: 1.21576654911\n",
      "Average cost: 0.139562351952\n",
      "Epoch 35 Learning rate: 1.09418988228\n",
      "Average cost: 0.142617507213\n",
      "Epoch 36 Learning rate: 0.984770894051\n",
      "Average cost: 0.130410060485\n",
      "Epoch 37 Learning rate: 0.886293828487\n",
      "Average cost: 0.113658136643\n",
      "Epoch 38 Learning rate: 0.797664403915\n",
      "Average cost: 0.11373493361\n",
      "Epoch 39 Learning rate: 0.717898011208\n",
      "Average cost: 0.118335420862\n",
      "Epoch 40 Learning rate: 0.646108210087\n",
      "Average cost: 0.111798206286\n",
      "Epoch 41 Learning rate: 0.581497371197\n",
      "Average cost: 0.107308442775\n",
      "Epoch 42 Learning rate: 0.523347616196\n",
      "Average cost: 0.112230280555\n",
      "Epoch 43 Learning rate: 0.471012860537\n",
      "Average cost: 0.115452847911\n",
      "Epoch 44 Learning rate: 0.423911571503\n",
      "Average cost: 0.108720129348\n",
      "Epoch 45 Learning rate: 0.381520420313\n",
      "Average cost: 0.111122766766\n",
      "Epoch 46 Learning rate: 0.343368381262\n",
      "Average cost: 0.113805787083\n",
      "Epoch 47 Learning rate: 0.309031546116\n",
      "Average cost: 0.112868506743\n",
      "Epoch 48 Learning rate: 0.278128385544\n",
      "Average cost: 0.103680121634\n",
      "Epoch 49 Learning rate: 0.250315546989\n",
      "Average cost: 0.0979806693229\n",
      "Epoch 50 Learning rate: 0.225283995271\n",
      "Average cost: 0.102047023707\n",
      "Epoch 51 Learning rate: 0.202755600214\n",
      "Average cost: 0.0993250492546\n",
      "Epoch 52 Learning rate: 0.182480037212\n",
      "Average cost: 0.0927380908363\n",
      "Epoch 53 Learning rate: 0.164232030511\n",
      "Average cost: 0.0978978193137\n",
      "Epoch 54 Learning rate: 0.14780883491\n",
      "Average cost: 0.0942342063536\n",
      "Epoch 55 Learning rate: 0.133027940989\n",
      "Average cost: 0.0905814168602\n",
      "Epoch 56 Learning rate: 0.11972515285\n",
      "Average cost: 0.0896438352101\n",
      "Epoch 57 Learning rate: 0.107752636075\n",
      "Average cost: 0.0922508786039\n",
      "Epoch 58 Learning rate: 0.0969773754478\n",
      "Average cost: 0.0917611687796\n",
      "Epoch 59 Learning rate: 0.0872796326876\n",
      "Average cost: 0.090726270568\n",
      "Epoch 60 Learning rate: 0.078551672399\n",
      "Average cost: 0.0890468636735\n",
      "Epoch 61 Learning rate: 0.070696502924\n",
      "Average cost: 0.0851835879601\n",
      "Epoch 62 Learning rate: 0.0636268556118\n",
      "Average cost: 0.0856278891116\n",
      "Epoch 63 Learning rate: 0.0572641678154\n",
      "Average cost: 0.0847257202698\n",
      "Epoch 64 Learning rate: 0.0515377521515\n",
      "Average cost: 0.0872937794526\n",
      "Epoch 65 Learning rate: 0.0463839769363\n",
      "Average cost: 0.0867779570156\n",
      "Epoch 66 Learning rate: 0.0417455807328\n",
      "Average cost: 0.0839792846888\n",
      "Epoch 67 Learning rate: 0.0375710204244\n",
      "Average cost: 0.0852443347871\n",
      "Epoch 68 Learning rate: 0.033813919872\n",
      "Average cost: 0.0842530752222\n",
      "Epoch 69 Learning rate: 0.0304325278848\n",
      "Average cost: 0.0844835746206\n",
      "Epoch 70 Learning rate: 0.0273892749101\n",
      "Average cost: 0.0833380839643\n",
      "Epoch 71 Learning rate: 0.0246503464878\n",
      "Average cost: 0.084131051393\n",
      "Epoch 72 Learning rate: 0.022185312584\n",
      "Average cost: 0.0856830258999\n",
      "Epoch 73 Learning rate: 0.0199667811394\n",
      "Average cost: 0.0855502404273\n",
      "Epoch 74 Learning rate: 0.0179701037705\n",
      "Average cost: 0.0872696110192\n",
      "Epoch 75 Learning rate: 0.0161730926484\n",
      "Average cost: 0.0862733836389\n",
      "Epoch 76 Learning rate: 0.014555783011\n",
      "Average cost: 0.0861652084937\n",
      "Epoch 77 Learning rate: 0.0131002049893\n",
      "Average cost: 0.0868757864585\n",
      "Epoch 78 Learning rate: 0.0117901843041\n",
      "Average cost: 0.0856073559655\n",
      "Epoch 79 Learning rate: 0.0106111662462\n",
      "Average cost: 0.086159056392\n",
      "Epoch 80 Learning rate: 0.00955004990101\n",
      "Average cost: 0.0854317350934\n",
      "Epoch 81 Learning rate: 0.00859504472464\n",
      "Average cost: 0.0858978029837\n",
      "Epoch 82 Learning rate: 0.00773554015905\n",
      "Average cost: 0.0849054252356\n",
      "Epoch 83 Learning rate: 0.00696198595688\n",
      "Average cost: 0.0853864532544\n",
      "Epoch 84 Learning rate: 0.00626578740776\n",
      "Average cost: 0.0857825528168\n",
      "Epoch 85 Learning rate: 0.00563920894638\n",
      "Average cost: 0.0852726992302\n",
      "Epoch 86 Learning rate: 0.00507528800517\n",
      "Average cost: 0.0853100729237\n",
      "Epoch 87 Learning rate: 0.00456775911152\n",
      "Average cost: 0.0855984703534\n",
      "Epoch 88 Learning rate: 0.00411098310724\n",
      "Average cost: 0.0855567909694\n",
      "Epoch 89 Learning rate: 0.00369988474995\n",
      "Average cost: 0.0850416279833\n",
      "Epoch 90 Learning rate: 0.00332989636809\n",
      "Average cost: 0.0851050570938\n",
      "Epoch 91 Learning rate: 0.00299690663815\n",
      "Average cost: 0.085178633324\n",
      "Epoch 92 Learning rate: 0.00269721611403\n",
      "Average cost: 0.0850105406096\n",
      "Epoch 93 Learning rate: 0.00242749438621\n",
      "Average cost: 0.0847941023691\n",
      "Epoch 94 Learning rate: 0.00218474492431\n",
      "Average cost: 0.0842993930644\n",
      "Epoch 95 Learning rate: 0.00196627061814\n",
      "Average cost: 0.0842550259911\n",
      "Epoch 96 Learning rate: 0.00176964350976\n",
      "Average cost: 0.0844948678629\n",
      "Epoch 97 Learning rate: 0.00159267906565\n",
      "Average cost: 0.0850287980172\n",
      "Epoch 98 Learning rate: 0.00143341114745\n",
      "Average cost: 0.0849060964833\n",
      "Epoch 99 Learning rate: 0.00129007012583\n",
      "Average cost: 0.0850990822166\n",
      "Epoch 100 Learning rate: 0.00116106308997\n",
      "Average cost: 0.0847728715589\n",
      "Epoch 101 Learning rate: 0.0010449567344\n",
      "Average cost: 0.0848129388359\n",
      "Epoch 102 Learning rate: 0.000940461060964\n",
      "Average cost: 0.0840571327011\n",
      "Epoch 103 Learning rate: 0.000846415001433\n",
      "Average cost: 0.0841821643462\n",
      "Epoch 104 Learning rate: 0.000761773495469\n",
      "Average cost: 0.0843714795096\n",
      "Epoch 105 Learning rate: 0.000685596140102\n",
      "Average cost: 0.0844191122966\n",
      "Epoch 106 Learning rate: 0.000617036537733\n",
      "Average cost: 0.0840176563048\n",
      "Epoch 107 Learning rate: 0.000555332866497\n",
      "Average cost: 0.0840031479961\n",
      "Epoch 108 Learning rate: 0.000499799556565\n",
      "Average cost: 0.0843830711643\n",
      "Epoch 109 Learning rate: 0.000449819635833\n",
      "Average cost: 0.0841006109201\n",
      "Epoch 110 Learning rate: 0.000404837657697\n",
      "Average cost: 0.083910044233\n",
      "Epoch 111 Learning rate: 0.000364353880286\n",
      "Average cost: 0.0838914345039\n",
      "Epoch 112 Learning rate: 0.000327918503899\n",
      "Average cost: 0.0836083757795\n",
      "Epoch 113 Learning rate: 0.000295126665151\n",
      "Average cost: 0.0837407027019\n",
      "Epoch 114 Learning rate: 0.000265613984084\n",
      "Average cost: 0.0836034374187\n",
      "Epoch 115 Learning rate: 0.000239052591496\n",
      "Average cost: 0.0833829758565\n",
      "Epoch 116 Learning rate: 0.000215147330891\n",
      "Average cost: 0.0834241389649\n",
      "Epoch 117 Learning rate: 0.000193632600713\n",
      "Average cost: 0.0832396215863\n",
      "Epoch 118 Learning rate: 0.000174269342097\n",
      "Average cost: 0.0833750714031\n",
      "Epoch 119 Learning rate: 0.000156842404976\n",
      "Average cost: 0.0832041196856\n",
      "Epoch 120 Learning rate: 0.000141158161568\n",
      "Average cost: 0.0830037731429\n",
      "Epoch 121 Learning rate: 0.000127042352688\n",
      "Average cost: 0.0829103712986\n",
      "Epoch 122 Learning rate: 0.000114338115964\n",
      "Average cost: 0.0827247463498\n",
      "Epoch 123 Learning rate: 0.000102904297819\n",
      "Average cost: 0.0827175776329\n",
      "Epoch 124 Learning rate: 9.26138745854e-05\n",
      "Average cost: 0.0826936955419\n",
      "Epoch 125 Learning rate: 8.33524827613e-05\n",
      "Average cost: 0.0827778677477\n",
      "Epoch 126 Learning rate: 7.50172330299e-05\n",
      "Average cost: 0.0827625995295\n",
      "Epoch 127 Learning rate: 6.75155097269e-05\n",
      "Average cost: 0.0826774219755\n",
      "Epoch 128 Learning rate: 6.07639594818e-05\n",
      "Average cost: 0.0825573724426\n",
      "Epoch 129 Learning rate: 5.46875635337e-05\n",
      "Average cost: 0.0825372473068\n",
      "Epoch 130 Learning rate: 4.92188082717e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-af2fc9b41280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                             \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnext_batch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelatedness_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                             \u001b[0mdropout_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                             \u001b[0membedding_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minit_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                         })\n\u001b[1;32m     65\u001b[0m                     \u001b[0mavg_cost\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None,1]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        last_A=tf.transpose(outputs_A,[1,0,2])[-1]\n",
    "        last_B=tf.transpose(outputs_B,[1,0,2])[-1]\n",
    "        #concat_outputs=tf.concat(1,[last_A,last_B])\n",
    "        #fully_connected = tf.contrib.layers.fully_connected(concat_outputs,num_outputs=1,activation_fn=tf.tanh)\n",
    "        prediction=tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.abs(tf.sub(last_A,last_B)),1)))\n",
    "\n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        sv=tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "        with sv.managed_session() as sess:\n",
    "            total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "            print('Total batch size: {}'.format(total_batch))\n",
    "\n",
    "            # train\n",
    "            for epoch in range(config.max_max_epoch):\n",
    "                lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "                avg_cost=0.\n",
    "                for i in range(total_batch):\n",
    "                    start=i*config.batch_size\n",
    "                    end=(i+1)*config.batch_size\n",
    "\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                    \n",
    "                print('Average cost: {}'.format(avg_cost/total_batch))\n",
    "\n",
    "            # validation\n",
    "            valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                sentences_A:valid_data.sentences_A,\n",
    "                sentencesA_length:valid_data.sentencesA_length,\n",
    "                sentences_B:valid_data.sentences_B,\n",
    "                sentencesB_length:valid_data.sentencesB_length,\n",
    "                labels:np.reshape(valid_data.relatedness_scores,(len(valid_data.relatedness_scores),1)),\n",
    "                embedding_placeholder:init_W,\n",
    "                dropout_f:1.0\n",
    "            })\n",
    "            print(valid_cost)\n",
    "            \n",
    "            if FLAGS.save_path:\n",
    "                print('Saving model to {}'.format(FLAGS.save_path))\n",
    "                sv.saver.save(sess,FLAGS.save_path,global_step=sv.global_step)\n",
    "\n",
    "            # test\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "                sentences_A:test_data.sentences_A,\n",
    "                sentencesA_length:test_data.sentencesA_length,\n",
    "                sentences_B:test_data.sentences_B,\n",
    "                sentencesB_length:test_data.sentencesB_length,\n",
    "                labels:np.reshape(test_data.relatedness_scores,(len(test_data.relatedness_scores),1)),\n",
    "                embedding_placeholder:init_W,\n",
    "                dropout_f:1.0\n",
    "            })\n",
    "            print(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SICK/sts_test_result.txt','w') as fw:\n",
    "    fw.write('pair_ID\trelatedness_score\tentailment_judgment\\n')\n",
    "    for _ in range(len(test_predict)):\n",
    "        fw.write(test_data.pairIDs[_]+'\\t'+str(test_predict[_]*4+1)+'\\tNA\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
