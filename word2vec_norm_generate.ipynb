{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path,words):\n",
    "    print('Load word2vec file {}'.format(word2vec_path))\n",
    "    with open(word2vec_path,'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer2_size=map(int,header.split())\n",
    "        print('vocab_size={}'.format(vocab_size))\n",
    "\n",
    "        binary_len=np.dtype('float32').itemsize*300\n",
    "\n",
    "        with open('word2vec_norm.txt','w') as fw:\n",
    "            for _ in range(vocab_size):\n",
    "                word=[]\n",
    "                while True:\n",
    "                    ch=f.read(1)\n",
    "                    if ch==' ':\n",
    "                        word=''.join(word)\n",
    "                        break\n",
    "                    if ch!='\\n':\n",
    "                        word.append(ch)\n",
    "                word=word.decode('utf-8')\n",
    "                if word in words:\n",
    "                    fw.write(word)\n",
    "                    for val in np.fromstring(f.read(binary_len), dtype='float32'):\n",
    "                        fw.write(' {}'.format(val))\n",
    "                    fw.write('\\n')\n",
    "                else:\n",
    "                    f.read(binary_len)\n",
    "\n",
    "def file_to_words(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        words=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            for word in sentence_A.split():\n",
    "                if word not in words:\n",
    "                    words.append(word)\n",
    "\n",
    "            for word in sentence_B.split():\n",
    "                if word not in words:\n",
    "                    words.append(word)\n",
    "    return words\n",
    "\n",
    "word2vec_path='embeddings/GoogleNews-vectors-negative300.bin'\n",
    "words=file_to_words('SICK/SICK_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec file embeddings/GoogleNews-vectors-negative300.bin\n",
      "vocab_size=3000000\n"
     ]
    }
   ],
   "source": [
    "build_vocab(word2vec_path,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SICK/SICK_all_train.txt','r') as f:\n",
    "    header=f.readline()\n",
    "    sentences=[]\n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if not line: break\n",
    "        sentences.append(line)\n",
    "    shuffle(sentences)\n",
    "    with open('SICK/SICK_new_trial.txt','w') as fw:\n",
    "        fw.write(header)\n",
    "        for item in sentences[:1500]:\n",
    "            fw.write(item)\n",
    "    with open('SICK/SICK_new_train.txt','w') as fw:\n",
    "        fw.write(header)\n",
    "        for item in sentences[1500:]:\n",
    "            fw.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(senna_path,senna_embeddings_path,words):\n",
    "    count=0\n",
    "    senna_words=[]\n",
    "    embeddings=[]\n",
    "    with open(senna_path,'r') as f:\n",
    "        while True:\n",
    "            word=f.readline()[:-1]\n",
    "            if not word: break\n",
    "            if word in words: count+=1\n",
    "            senna_words.append(word)\n",
    "            \n",
    "    with open(senna_embeddings_path,'r') as f:\n",
    "        while True:\n",
    "            embedding=f.readline()\n",
    "            if not embedding: break\n",
    "            embeddings.append(embedding)\n",
    "                \n",
    "    emb_dictionary = dict(zip(senna_words, embeddings))\n",
    "\n",
    "    with open('embeddings/senna_norm.txt','w') as fw:\n",
    "        fw.write(str(count)+' 50\\n')\n",
    "        for key, value in emb_dictionary.iteritems():\n",
    "            if key in words:\n",
    "                fw.write(key+' '+value)\n",
    "\n",
    "def file_to_words(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        words=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            for word in sentence_A.split():\n",
    "                if word.lower() not in words:\n",
    "                    words.append(word.lower())\n",
    "\n",
    "            for word in sentence_B.split():\n",
    "                if word.lower() not in words:\n",
    "                    words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "senna_path='embeddings/data2_v_senna.txt'\n",
    "senna_embeddings_path='embeddings/data2_wordvector_senna.txt'\n",
    "words=file_to_words('SICK/SICK_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_vocab(senna_path,senna_embeddings_path,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
