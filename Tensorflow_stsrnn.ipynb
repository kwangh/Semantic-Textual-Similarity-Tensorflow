{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "data_path=SICK\n",
      "embedding_dim=300\n",
      "max_length=26\n",
      "save_path=SICK/STS_log/\n",
      "use_fp64=False\n",
      "word2vec_norm=embeddings/word2vec_norm.txt\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_norm','embeddings/word2vec_norm.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','SICK','SICK data set path')\n",
    "flags.DEFINE_string('save_path','SICK/STS_log/','STS model output directory')\n",
    "flags.DEFINE_integer('embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('max_length',26,'one sentence max length words which is in dictionary')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path=None):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            vocab_size,layer2_size=map(int,header.split())\n",
    "            # initial matrix with random uniform\n",
    "            init_W=np.random.uniform(-0.25,0.25,(vocab_size,FLAGS.embedding_dim))\n",
    "\n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            dictionary=dict()\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                dictionary[word]=len(dictionary)\n",
    "                init_W[dictionary[word]]=np.array(line.split()[1:], dtype=np.float32)\n",
    "\n",
    "        return dictionary,init_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_senna_word_ids(filename,word_to_id,is_test=False):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        pairIDs=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            ID=line.split('\\t')[0] # for test\n",
    "            pairIDs.append(ID)\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]\n",
    "            \n",
    "            _=[word_to_id[word.lower()] for word in sentence_A.split() if word.lower() in word_to_id]\n",
    "            sentencesA_length.append(len(_)) # must be before [0]*(FLAGS.max_length-len(_))\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_A.append(_)\n",
    "            \n",
    "            _=[word_to_id[word.lower()] for word in sentence_B.split() if word.lower() in word_to_id]\n",
    "            sentencesB_length.append(len(_))\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_B.append(_)\n",
    "            \n",
    "            relatedness_scores.append((float(relatedness_score)-1)/4)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    if not is_test: return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "    else:\n",
    "        stsinput=STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "        stsinput.pairIDs=pairIDs\n",
    "        return stsinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_word2vec_word_ids(filename,word_to_id,is_test=False):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        pairIDs=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            ID=line.split('\\t')[0] # for test\n",
    "            pairIDs.append(ID)\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            \n",
    "            _=[word_to_id[word] for word in sentence_A.split() if word in word_to_id]\n",
    "            sentencesA_length.append(len(_)) # must be before [0]*(FLAGS.max_length-len(_))\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_A.append(_)\n",
    "            \n",
    "            _=[word_to_id[word] for word in sentence_B.split() if word in word_to_id]\n",
    "            sentencesB_length.append(len(_))\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_B.append(_)\n",
    "            \n",
    "            relatedness_scores.append((float(relatedness_score)-1)/4)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    if not is_test: return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "    else:\n",
    "        stsinput=STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "        stsinput.pairIDs=pairIDs\n",
    "        return stsinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSInput(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec_norm file embeddings/word2vec_norm.txt\n",
      "vocab_size=2378\n"
     ]
    }
   ],
   "source": [
    "train_path=os.path.join(FLAGS.data_path,'SICK_new_train.txt')\n",
    "valid_path=os.path.join(FLAGS.data_path,'SICK_new_trial.txt')\n",
    "test_path=os.path.join(FLAGS.data_path,'SICK_test_annotated.txt')\n",
    "\n",
    "dictionary,init_W=build_vocab(FLAGS.word2vec_norm)\n",
    "train_data=file_to_word2vec_word_ids(train_path,dictionary)\n",
    "valid_data=file_to_word2vec_word_ids(valid_path,dictionary,is_test=True)\n",
    "test_data=file_to_word2vec_word_ids(test_path,dictionary,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(start,end,input):\n",
    "    inputs_A=input.sentences_A[start:end]\n",
    "    inputsA_length=input.sentencesA_length[start:end]\n",
    "    inputs_B=input.sentences_B[start:end]\n",
    "    inputsB_length=input.sentencesB_length[start:end]\n",
    "    labels=np.reshape(input.relatedness_scores[start:end],(-1))\n",
    "    return STSInput(inputs_A,inputsA_length,inputs_B,inputsB_length,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    init_scale=0.1\n",
    "    learning_rate=.001\n",
    "    max_grad_norm=1\n",
    "    keep_prob=1.\n",
    "    lr_decay=0.98\n",
    "    batch_size=30\n",
    "    max_epoch=15\n",
    "    max_max_epoch=66\n",
    "    \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_,input_length,dropout_):\n",
    "    rnn_cell=tf.nn.rnn_cell.BasicRNNCell(num_units=50)\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*2)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model/siamese/RNN/LSTMCell/W_0:0\n",
      "Model/siamese/RNN/LSTMCell/B:0\n",
      "Model/fully_connected/weights:0\n",
      "Model/fully_connected/biases:0\n",
      "Total batch size: 116, data size: 3500, batch size: 30\n",
      "0.001 1 1.0 0.98 15 66\n",
      "Epoch 0 Learning rate: 0.0010000000475\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [120] vs. [30]\n\t [[Node: Model/Sub = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Model/Sigmoid, _recv_Model/relatedness_score_label_0/_19)]]\n\t [[Node: Model/Adam/update/_92 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_792_Model/Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Model/Sub', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-170-33f8a9f97ee7>\", line 31, in <module>\n    cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2368, in sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [120] vs. [30]\n\t [[Node: Model/Sub = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Model/Sigmoid, _recv_Model/relatedness_score_label_0/_19)]]\n\t [[Node: Model/Adam/update/_92 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_792_Model/Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-33f8a9f97ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                             \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnext_batch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelatedness_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                             \u001b[0mdropout_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                             \u001b[0membedding_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minit_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                         })\n\u001b[1;32m     76\u001b[0m                     \u001b[0mavg_cost\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [120] vs. [30]\n\t [[Node: Model/Sub = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Model/Sigmoid, _recv_Model/relatedness_score_label_0/_19)]]\n\t [[Node: Model/Adam/update/_92 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_792_Model/Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Model/Sub', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-170-33f8a9f97ee7>\", line 31, in <module>\n    cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2368, in sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [120] vs. [30]\n\t [[Node: Model/Sub = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Model/Sigmoid, _recv_Model/relatedness_score_label_0/_19)]]\n\t [[Node: Model/Adam/update/_92 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_792_Model/Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    #initializer=tf.random_normal_initializer(-config.init_scale,config.init_scale)\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        # model\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        concat_outputs=tf.concat(1,[last_states_A,last_states_B])\n",
    "        fully_connected = tf.contrib.layers.fully_connected(concat_outputs,num_outputs=1,activation_fn=tf.tanh,biases_initializer=initializer)\n",
    "        fully_connected=tf.reshape(fully_connected,[-1])\n",
    "        prediction=tf.sigmoid(fully_connected)\n",
    "        \n",
    "        # cost\n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        #optimizer=tf.train.AdadeltaOptimizer(learning_rate=config.learning_rate)\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=config.learning_rate)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        for v in tf.trainable_variables():\n",
    "            print(v.name)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "            print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(train_data.sentences_A),config.batch_size))\n",
    "            print(config.learning_rate,config.max_grad_norm,config.keep_prob,config.lr_decay,config.max_epoch,config.max_max_epoch)\n",
    "            # train\n",
    "            prev_train_cost=1\n",
    "            prev_valid_cost=1\n",
    "            for epoch in range(config.max_max_epoch):\n",
    "                lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                #print('Epoch {} Learning rate: {}'.format(epoch,config.learning_rate))\n",
    "                avg_cost=0.\n",
    "                \n",
    "                for i in range(total_batch):\n",
    "                    start=i*config.batch_size\n",
    "                    end=(i+1)*config.batch_size\n",
    "\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                    \n",
    "                start=total_batch * config.batch_size\n",
    "                end=len(train_data.sentences_A)\n",
    "                if not start==end:\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                \n",
    "                if prev_train_cost>avg_cost/total_batch: print('Average cost:\\t{} ↓'.format(avg_cost/total_batch))\n",
    "                else: print('Average cost:\\t{} ↑'.format(avg_cost/total_batch))\n",
    "                prev_train_cost=avg_cost/total_batch\n",
    "                \n",
    "                # validation\n",
    "                valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                    sentences_A:valid_data.sentences_A,\n",
    "                    sentencesA_length:valid_data.sentencesA_length,\n",
    "                    sentences_B:valid_data.sentences_B,\n",
    "                    sentencesB_length:valid_data.sentencesB_length,\n",
    "                    labels:np.reshape(valid_data.relatedness_scores,(-1)),\n",
    "                    embedding_placeholder:init_W,\n",
    "                    dropout_f:1.0\n",
    "                })\n",
    "                if prev_valid_cost>valid_cost: print('Valid cost:\\t{} ↓'.format(valid_cost))\n",
    "                else: print('Valid cost:\\t{} ↑'.format(valid_cost))\n",
    "                prev_valid_cost=valid_cost\n",
    "                \n",
    "            saver.save(sess, FLAGS.save_path+'stsrnn-model',global_step=config.max_max_epoch)\n",
    "\n",
    "            # test\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "                sentences_A:test_data.sentences_A,\n",
    "                sentencesA_length:test_data.sentencesA_length,\n",
    "                sentences_B:test_data.sentences_B,\n",
    "                sentencesB_length:test_data.sentencesB_length,\n",
    "                labels:np.reshape(test_data.relatedness_scores,(-1)),\n",
    "                embedding_placeholder:init_W,\n",
    "                dropout_f:1.0\n",
    "            })\n",
    "            print(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SICK/stsrnn_trial_result00.txt','w') as fw:\n",
    "    fw.write('pair_ID\trelatedness_score\tentailment_judgment\\n')\n",
    "    for _ in range(len(valid_predict)):\n",
    "        fw.write(valid_data.pairIDs[_]+'\\t'+str(valid_predict[_]*4+1)+'\\tNA\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20061688 -0.20061688 -0.20061688 ..., -0.20061688 -0.20061688\n",
      " -0.20061688]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        last_A=tf.transpose(outputs_A,[1,0,2])[-1]\n",
    "        last_B=tf.transpose(outputs_B,[1,0,2])[-1]\n",
    "        concat_outputs=tf.concat(1,[last_A,last_B])\n",
    "        fully_connected = tf.contrib.layers.fully_connected(concat_outputs,num_outputs=1,activation_fn=tf.tanh,biases_initializer=initializer)\n",
    "        fully_connected=tf.reshape(fully_connected,[-1])\n",
    "        prediction=tf.sigmoid(fully_connected)\n",
    "        \n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "#             saver = tf.train.Saver()\n",
    "#             total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "#             print('Total batch size: {}'.format(total_batch))\n",
    "            \n",
    "            lsA,senAemb,outA,laA,concat,fc,lb,pr,co=sess.run([last_states_A,sentences_A_emb,outputs_A,last_A,concat_outputs,fully_connected,labels,prediction,cost],feed_dict={\n",
    "                    sentences_A:train_data.sentences_A,\n",
    "                            sentencesA_length:train_data.sentencesA_length,\n",
    "                            sentences_B:train_data.sentences_B,\n",
    "                            sentencesB_length:train_data.sentencesB_length,\n",
    "                            labels:np.reshape(train_data.relatedness_scores,(-1)),\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                })\n",
    "            print(fc)\n",
    "#             # train\n",
    "#             for epoch in range(config.max_max_epoch):\n",
    "#                 lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "#                 sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "#                 print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "#                 avg_cost=0.\n",
    "#                 for i in range(total_batch):\n",
    "#                     start=i*config.batch_size\n",
    "#                     end=(i+1)*config.batch_size\n",
    "\n",
    "#                     next_batch_input=next_batch(start,end,train_data)\n",
    "#                     _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "#                             sentences_A:next_batch_input.sentences_A,\n",
    "#                             sentencesA_length:next_batch_input.sentencesA_length,\n",
    "#                             sentences_B:next_batch_input.sentences_B,\n",
    "#                             sentencesB_length:next_batch_input.sentencesB_length,\n",
    "#                             labels:next_batch_input.relatedness_scores,\n",
    "#                             dropout_f:config.keep_prob,\n",
    "#                             embedding_placeholder:init_W\n",
    "#                         })\n",
    "#                     avg_cost+=train_cost\n",
    "                    \n",
    "#                 print('Average cost: {}'.format(avg_cost/total_batch))\n",
    "                \n",
    "#                 # validation\n",
    "#                 valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                     sentences_A:valid_data.sentences_A,\n",
    "#                     sentencesA_length:valid_data.sentencesA_length,\n",
    "#                     sentences_B:valid_data.sentences_B,\n",
    "#                     sentencesB_length:valid_data.sentencesB_length,\n",
    "#                     labels:np.reshape(valid_data.relatedness_scores,(len(valid_data.relatedness_scores),1)),\n",
    "#                     embedding_placeholder:init_W,\n",
    "#                     dropout_f:1.0\n",
    "#                 })\n",
    "#                 print('Valid cost: {}'.format(valid_cost))\n",
    "                \n",
    "#             saver.save(sess, 'SICK/STS_log/stsrnn-model', global_step=max_max_epoch)\n",
    "\n",
    "#             # test\n",
    "#             test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                 sentences_A:test_data.sentences_A,\n",
    "#                 sentencesA_length:test_data.sentencesA_length,\n",
    "#                 sentences_B:test_data.sentences_B,\n",
    "#                 sentencesB_length:test_data.sentencesB_length,\n",
    "#                 labels:np.reshape(test_data.relatedness_scores,(len(test_data.relatedness_scores),1)),\n",
    "#                 embedding_placeholder:init_W,\n",
    "#                 dropout_f:1.0\n",
    "#             })\n",
    "#             print(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.39466038,  0.34863734,  0.2043025 , ...,  0.2695404 ,\n",
       "          0.35967061, -0.07278638],\n",
       "        [ 0.19325171,  0.21964903, -0.01311609, ...,  0.11723963,\n",
       "         -0.07891562,  0.04409149],\n",
       "        [-0.04842308, -0.11699392,  0.21643849, ...,  0.45521411,\n",
       "          0.11933228,  0.15362358],\n",
       "        ..., \n",
       "        [-0.07463912,  0.09813502, -0.22187519, ...,  0.28413203,\n",
       "          0.21442533, -0.14031124],\n",
       "        [-0.00901718, -0.29061171, -0.00509488, ..., -0.19280693,\n",
       "         -0.1929002 , -0.05673572],\n",
       "        [ 0.29807368,  0.11062274, -0.1031251 , ...,  0.30396405,\n",
       "         -0.04571057,  0.13657165]], dtype=float32),\n",
       " array([[-0.32004598,  0.07898287, -0.14715722, ...,  0.1460098 ,\n",
       "         -0.47885492, -0.20278063],\n",
       "        [ 0.18979396,  0.28211758, -0.12481226, ...,  0.49265289,\n",
       "          0.15833965,  0.06163326],\n",
       "        [-0.23689494, -0.04968107, -0.16944048, ...,  0.36214861,\n",
       "         -0.35104766, -0.21938179],\n",
       "        ..., \n",
       "        [ 0.10285117,  0.41628623, -0.37386632, ...,  0.19995578,\n",
       "         -0.32830602,  0.12629585],\n",
       "        [ 0.30783314, -0.00979346, -0.04624661, ...,  0.02585384,\n",
       "          0.41611612, -0.1559044 ],\n",
       "        [-0.37925678,  0.39741862, -0.24355973, ..., -0.12644655,\n",
       "         -0.08932327,  0.08850812]], dtype=float32))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
