{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "data_path=SICK\n",
      "embedding_dim=300\n",
      "max_length=26\n",
      "save_path=SICK/STS_log/\n",
      "use_fp64=False\n",
      "word2vec_norm=embeddings/word2vec_norm.txt\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_norm','embeddings/word2vec_norm.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','SICK','SICK data set path')\n",
    "flags.DEFINE_string('save_path','SICK/STS_log/','STS model output directory')\n",
    "flags.DEFINE_integer('embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('max_length',26,'one sentence max length words which is in dictionary')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(word2vec_path=None):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            vocab_size,layer2_size=map(int,header.split())\n",
    "            # initial matrix with random uniform\n",
    "            init_W=np.random.uniform(-0.25,0.25,(vocab_size,FLAGS.embedding_dim))\n",
    "\n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            dictionary=dict()\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                dictionary[word]=len(dictionary)\n",
    "                init_W[dictionary[word]]=np.array(line.split()[1:], dtype=np.float32)\n",
    "\n",
    "        return dictionary,init_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename,word_to_id,is_test=False):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # remove header\n",
    "        sentences_A=[]\n",
    "        sentencesA_length=[]\n",
    "        sentences_B=[]\n",
    "        sentencesB_length=[]\n",
    "        relatedness_scores=[]\n",
    "        pairIDs=[]\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "            ID=line.split('\\t')[0] # for test\n",
    "            pairIDs.append(ID)\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            _=[word_to_id[word] for word in sentence_A.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_A.append(_)\n",
    "            sentencesA_length.append(len(_))\n",
    "            _=[word_to_id[word] for word in sentence_B.split() if word in word_to_id]\n",
    "            _+=[0]*(FLAGS.max_length-len(_))\n",
    "            sentences_B.append(_)\n",
    "            sentencesB_length.append(len(_))\n",
    "            relatedness_scores.append((float(relatedness_score)-1)/4)\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    if not is_test: return STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "    else:\n",
    "        stsinput=STSInput(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "        stsinput.pairIDs=pairIDs\n",
    "        return stsinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STSInput(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec_norm file embeddings/word2vec_norm.txt\n",
      "vocab_size=2378\n"
     ]
    }
   ],
   "source": [
    "train_path=os.path.join(FLAGS.data_path,'SICK_new_train.txt')\n",
    "valid_path=os.path.join(FLAGS.data_path,'SICK_new_trial.txt')\n",
    "test_path=os.path.join(FLAGS.data_path,'SICK_test_annotated.txt')\n",
    "\n",
    "dictionary,init_W=build_vocab(FLAGS.word2vec_norm)\n",
    "train_data=file_to_word_ids(train_path,dictionary)\n",
    "valid_data=file_to_word_ids(valid_path,dictionary,is_test=True)\n",
    "test_data=file_to_word_ids(test_path,dictionary,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(start,end,input):\n",
    "    inputs_A=input.sentences_A[start:end]\n",
    "    inputsA_length=input.sentencesA_length[start:end]\n",
    "    inputs_B=input.sentences_B[start:end]\n",
    "    inputsB_length=input.sentencesB_length[start:end]\n",
    "    labels=np.reshape(input.relatedness_scores[start:end],(-1))\n",
    "    return STSInput(inputs_A,inputsA_length,inputs_B,inputsB_length,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    #init_scale=0.1\n",
    "    learning_rate=5.\n",
    "    max_grad_norm=10\n",
    "    keep_prob=.2\n",
    "    lr_decay=0.98\n",
    "    batch_size=32\n",
    "    max_epoch=15\n",
    "    max_max_epoch=100\n",
    "    \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_,input_length,dropout_):\n",
    "    rnn_cell=tf.nn.rnn_cell.BasicRNNCell(num_units=50)\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    #rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell])\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model/siamese/RNN/BasicRNNCell/Linear/Matrix:0\n",
      "Model/siamese/RNN/BasicRNNCell/Linear/Bias:0\n",
      "Model/fully_connected/weights:0\n",
      "Model/fully_connected/biases:0\n",
      "Total batch size: 109, data size: 3500, batch size: 32\n",
      "5.0 10 0.2 0.98 100\n",
      "Epoch 0 Learning rate: 5.0\n",
      "Average cost:\t0.071876075153\n",
      "Valid cost:\t0.0654662027955\n",
      "Epoch 1 Learning rate: 5.0\n",
      "Average cost:\t0.069945152268\n",
      "Valid cost:\t0.0663246884942\n",
      "Epoch 2 Learning rate: 5.0\n",
      "Average cost:\t0.0696715429391\n",
      "Valid cost:\t0.0653832629323\n",
      "Epoch 3 Learning rate: 5.0\n",
      "Average cost:\t0.0694895286386\n",
      "Valid cost:\t0.0656489059329\n",
      "Epoch 4 Learning rate: 5.0\n",
      "Average cost:\t0.0691336172915\n",
      "Valid cost:\t0.0652442499995\n",
      "Epoch 5 Learning rate: 5.0\n",
      "Average cost:\t0.0693475475381\n",
      "Valid cost:\t0.0655800253153\n",
      "Epoch 6 Learning rate: 5.0\n",
      "Average cost:\t0.0685094234388\n",
      "Valid cost:\t0.0653022825718\n",
      "Epoch 7 Learning rate: 5.0\n",
      "Average cost:\t0.0683755264233\n",
      "Valid cost:\t0.0651346594095\n",
      "Epoch 8 Learning rate: 5.0\n",
      "Average cost:\t0.068041432854\n",
      "Valid cost:\t0.0652087330818\n",
      "Epoch 9 Learning rate: 5.0\n",
      "Average cost:\t0.0686826727122\n",
      "Valid cost:\t0.0655890479684\n",
      "Epoch 10 Learning rate: 5.0\n",
      "Average cost:\t0.0680175333053\n",
      "Valid cost:\t0.0652890652418\n",
      "Epoch 11 Learning rate: 5.0\n",
      "Average cost:\t0.0677778662242\n",
      "Valid cost:\t0.0654445514083\n",
      "Epoch 12 Learning rate: 5.0\n",
      "Average cost:\t0.0680534334665\n",
      "Valid cost:\t0.06523578614\n",
      "Epoch 13 Learning rate: 5.0\n",
      "Average cost:\t0.0678638174378\n",
      "Valid cost:\t0.065121255815\n",
      "Epoch 14 Learning rate: 5.0\n",
      "Average cost:\t0.0676708449909\n",
      "Valid cost:\t0.0651607066393\n",
      "Epoch 15 Learning rate: 4.90000009537\n",
      "Average cost:\t0.0674987577004\n",
      "Valid cost:\t0.0652215257287\n",
      "Epoch 16 Learning rate: 4.80200004578\n",
      "Average cost:\t0.0670150039769\n",
      "Valid cost:\t0.0651767104864\n",
      "Epoch 17 Learning rate: 4.70595979691\n",
      "Average cost:\t0.0667297287818\n",
      "Valid cost:\t0.0652132853866\n",
      "Epoch 18 Learning rate: 4.61184072495\n",
      "Average cost:\t0.0670121410229\n",
      "Valid cost:\t0.0652887225151\n",
      "Epoch 19 Learning rate: 4.51960420609\n",
      "Average cost:\t0.0666298473237\n",
      "Valid cost:\t0.0651827827096\n",
      "Epoch 20 Learning rate: 4.42921209335\n",
      "Average cost:\t0.0668451101989\n",
      "Valid cost:\t0.0651655867696\n",
      "Epoch 21 Learning rate: 4.34062767029\n",
      "Average cost:\t0.066887387066\n",
      "Valid cost:\t0.0652439445257\n",
      "Epoch 22 Learning rate: 4.2538151741\n",
      "Average cost:\t0.0666153814063\n",
      "Valid cost:\t0.0651705563068\n",
      "Epoch 23 Learning rate: 4.16873884201\n",
      "Average cost:\t0.0666901040993\n",
      "Valid cost:\t0.0651787295938\n",
      "Epoch 24 Learning rate: 4.0853638649\n",
      "Average cost:\t0.066755111372\n",
      "Valid cost:\t0.0651355758309\n",
      "Epoch 25 Learning rate: 4.00365686417\n",
      "Average cost:\t0.0664078719367\n",
      "Valid cost:\t0.0652143880725\n",
      "Epoch 26 Learning rate: 3.92358350754\n",
      "Average cost:\t0.0667248645951\n",
      "Valid cost:\t0.0651691928506\n",
      "Epoch 27 Learning rate: 3.84511184692\n",
      "Average cost:\t0.0665507178173\n",
      "Valid cost:\t0.0651955455542\n",
      "Epoch 28 Learning rate: 3.76820969582\n",
      "Average cost:\t0.0660833326433\n",
      "Valid cost:\t0.0651830583811\n",
      "Epoch 29 Learning rate: 3.69284558296\n",
      "Average cost:\t0.0661275832991\n",
      "Valid cost:\t0.0651885420084\n",
      "Epoch 30 Learning rate: 3.61898851395\n",
      "Average cost:\t0.0661699291865\n",
      "Valid cost:\t0.0652017891407\n",
      "Epoch 31 Learning rate: 3.54660892487\n",
      "Average cost:\t0.0662530290834\n",
      "Valid cost:\t0.065207131207\n",
      "Epoch 32 Learning rate: 3.47567653656\n",
      "Average cost:\t0.066451900425\n",
      "Valid cost:\t0.0651644989848\n",
      "Epoch 33 Learning rate: 3.40616321564\n",
      "Average cost:\t0.0665595081695\n",
      "Valid cost:\t0.0651646703482\n",
      "Epoch 34 Learning rate: 3.33803987503\n",
      "Average cost:\t0.0660760826132\n",
      "Valid cost:\t0.0652354359627\n",
      "Epoch 35 Learning rate: 3.2712790966\n",
      "Average cost:\t0.066190820512\n",
      "Valid cost:\t0.0651254802942\n",
      "Epoch 36 Learning rate: 3.20585346222\n",
      "Average cost:\t0.0660854153882\n",
      "Valid cost:\t0.0651876628399\n",
      "Epoch 37 Learning rate: 3.14173650742\n",
      "Average cost:\t0.0663127938132\n",
      "Valid cost:\t0.0652588009834\n",
      "Epoch 38 Learning rate: 3.07890176773\n",
      "Average cost:\t0.0663789128628\n",
      "Valid cost:\t0.0652277618647\n",
      "Epoch 39 Learning rate: 3.01732373238\n",
      "Average cost:\t0.0661642073628\n",
      "Valid cost:\t0.0652036294341\n",
      "Epoch 40 Learning rate: 2.95697712898\n",
      "Average cost:\t0.066290381561\n",
      "Valid cost:\t0.0652181059122\n",
      "Epoch 41 Learning rate: 2.89783763885\n",
      "Average cost:\t0.066089022973\n",
      "Valid cost:\t0.0651770606637\n",
      "Epoch 42 Learning rate: 2.8398809433\n",
      "Average cost:\t0.0662440132828\n",
      "Valid cost:\t0.0652696341276\n",
      "Epoch 43 Learning rate: 2.78308320045\n",
      "Average cost:\t0.0661802147472\n",
      "Valid cost:\t0.065190628171\n",
      "Epoch 44 Learning rate: 2.72742152214\n",
      "Average cost:\t0.0660227633022\n",
      "Valid cost:\t0.0652358084917\n",
      "Epoch 45 Learning rate: 2.67287325859\n",
      "Average cost:\t0.0661873312992\n",
      "Valid cost:\t0.065178245306\n",
      "Epoch 46 Learning rate: 2.61941576004\n",
      "Average cost:\t0.0663182312332\n",
      "Valid cost:\t0.0652064308524\n",
      "Epoch 47 Learning rate: 2.5670273304\n",
      "Average cost:\t0.06613556405\n",
      "Valid cost:\t0.0652122795582\n",
      "Epoch 48 Learning rate: 2.51568675041\n",
      "Average cost:\t0.0664252238604\n",
      "Valid cost:\t0.0651632994413\n",
      "Epoch 49 Learning rate: 2.46537303925\n",
      "Average cost:\t0.0659511257134\n",
      "Valid cost:\t0.0650496408343\n",
      "Epoch 50 Learning rate: 2.4160656929\n",
      "Average cost:\t0.0660490322209\n",
      "Valid cost:\t0.0649304240942\n",
      "Epoch 51 Learning rate: 2.3677444458\n",
      "Average cost:\t0.0656901875152\n",
      "Valid cost:\t0.0647651702166\n",
      "Epoch 52 Learning rate: 2.3203895092\n",
      "Average cost:\t0.0656001699764\n",
      "Valid cost:\t0.0658737421036\n",
      "Epoch 53 Learning rate: 2.2739815712\n",
      "Average cost:\t0.065558525708\n",
      "Valid cost:\t0.0656131654978\n",
      "Epoch 54 Learning rate: 2.22850203514\n",
      "Average cost:\t0.0662013177649\n",
      "Valid cost:\t0.065248914063\n",
      "Epoch 55 Learning rate: 2.18393206596\n",
      "Average cost:\t0.0650988384705\n",
      "Valid cost:\t0.0663572847843\n",
      "Epoch 56 Learning rate: 2.14025330544\n",
      "Average cost:\t0.0643216321638\n",
      "Valid cost:\t0.0667585656047\n",
      "Epoch 57 Learning rate: 2.097448349\n",
      "Average cost:\t0.063624365409\n",
      "Valid cost:\t0.066965714097\n",
      "Epoch 58 Learning rate: 2.05549931526\n",
      "Average cost:\t0.0640361463873\n",
      "Valid cost:\t0.0669179707766\n",
      "Epoch 59 Learning rate: 2.0143892765\n",
      "Average cost:\t0.0629025568519\n",
      "Valid cost:\t0.068271741271\n",
      "Epoch 60 Learning rate: 1.97410154343\n",
      "Average cost:\t0.0625535720669\n",
      "Valid cost:\t0.0668526291847\n",
      "Epoch 61 Learning rate: 1.93461954594\n",
      "Average cost:\t0.0610936788347\n",
      "Valid cost:\t0.0695313587785\n",
      "Epoch 62 Learning rate: 1.89592707157\n",
      "Average cost:\t0.0618207194082\n",
      "Valid cost:\t0.0687338113785\n",
      "Epoch 63 Learning rate: 1.85800862312\n",
      "Average cost:\t0.0609079551533\n",
      "Valid cost:\t0.0676785707474\n",
      "Epoch 64 Learning rate: 1.82084834576\n",
      "Average cost:\t0.060583103619\n",
      "Valid cost:\t0.0674722567201\n",
      "Epoch 65 Learning rate: 1.78443145752\n",
      "Average cost:\t0.0598856121336\n",
      "Valid cost:\t0.0682309791446\n",
      "Epoch 66 Learning rate: 1.74874281883\n",
      "Average cost:\t0.0595724860873\n",
      "Valid cost:\t0.0688244402409\n",
      "Epoch 67 Learning rate: 1.71376800537\n",
      "Average cost:\t0.0591423376164\n",
      "Valid cost:\t0.0684514865279\n",
      "Epoch 68 Learning rate: 1.67949259281\n",
      "Average cost:\t0.057922217431\n",
      "Valid cost:\t0.0692322775722\n",
      "Epoch 69 Learning rate: 1.64590275288\n",
      "Average cost:\t0.0585909498462\n",
      "Valid cost:\t0.0694422572851\n",
      "Epoch 70 Learning rate: 1.61298465729\n",
      "Average cost:\t0.0577095431711\n",
      "Valid cost:\t0.070290133357\n",
      "Epoch 71 Learning rate: 1.58072495461\n",
      "Average cost:\t0.0571312823358\n",
      "Valid cost:\t0.0697338506579\n",
      "Epoch 72 Learning rate: 1.54911053181\n",
      "Average cost:\t0.0564531876851\n",
      "Valid cost:\t0.0696097910404\n",
      "Epoch 73 Learning rate: 1.51812827587\n",
      "Average cost:\t0.0567315386762\n",
      "Valid cost:\t0.0706086903811\n",
      "Epoch 74 Learning rate: 1.48776566982\n",
      "Average cost:\t0.0574214961978\n",
      "Valid cost:\t0.0694529041648\n",
      "Epoch 75 Learning rate: 1.4580104351\n",
      "Average cost:\t0.055167659502\n",
      "Valid cost:\t0.0698880329728\n",
      "Epoch 76 Learning rate: 1.42885017395\n",
      "Average cost:\t0.0564595875715\n",
      "Valid cost:\t0.0705795437098\n",
      "Epoch 77 Learning rate: 1.40027320385\n",
      "Average cost:\t0.0560281830541\n",
      "Valid cost:\t0.069761954248\n",
      "Epoch 78 Learning rate: 1.37226772308\n",
      "Average cost:\t0.0546096672364\n",
      "Valid cost:\t0.0697817206383\n",
      "Epoch 79 Learning rate: 1.34482240677\n",
      "Average cost:\t0.0554333443864\n",
      "Valid cost:\t0.0700225904584\n",
      "Epoch 80 Learning rate: 1.31792593002\n",
      "Average cost:\t0.0539581967398\n",
      "Valid cost:\t0.0698945745826\n",
      "Epoch 81 Learning rate: 1.2915674448\n",
      "Average cost:\t0.0539999511007\n",
      "Valid cost:\t0.0695032849908\n",
      "Epoch 82 Learning rate: 1.26573610306\n",
      "Average cost:\t0.0535364465771\n",
      "Valid cost:\t0.0695500895381\n",
      "Epoch 83 Learning rate: 1.24042129517\n",
      "Average cost:\t0.0539813966345\n",
      "Valid cost:\t0.0699904114008\n",
      "Epoch 84 Learning rate: 1.21561288834\n",
      "Average cost:\t0.0529321933319\n",
      "Valid cost:\t0.0699367672205\n",
      "Epoch 85 Learning rate: 1.19130063057\n",
      "Average cost:\t0.0527142462312\n",
      "Valid cost:\t0.0709997490048\n",
      "Epoch 86 Learning rate: 1.16747462749\n",
      "Average cost:\t0.0530138253961\n",
      "Valid cost:\t0.0707717686892\n",
      "Epoch 87 Learning rate: 1.14412510395\n",
      "Average cost:\t0.0525432438718\n",
      "Valid cost:\t0.0706483945251\n",
      "Epoch 88 Learning rate: 1.1212426424\n",
      "Average cost:\t0.0522616012962\n",
      "Valid cost:\t0.0705186799169\n",
      "Epoch 89 Learning rate: 1.09881782532\n",
      "Average cost:\t0.0515748591701\n",
      "Valid cost:\t0.0713287144899\n",
      "Epoch 90 Learning rate: 1.07684147358\n",
      "Average cost:\t0.0523817004175\n",
      "Valid cost:\t0.0709679722786\n",
      "Epoch 91 Learning rate: 1.05530464649\n",
      "Average cost:\t0.0515041440264\n",
      "Valid cost:\t0.0710002630949\n",
      "Epoch 92 Learning rate: 1.03419852257\n",
      "Average cost:\t0.0516904214858\n",
      "Valid cost:\t0.0705268681049\n",
      "Epoch 93 Learning rate: 1.01351451874\n",
      "Average cost:\t0.0508296107275\n",
      "Valid cost:\t0.0705368220806\n",
      "Epoch 94 Learning rate: 0.993244230747\n",
      "Average cost:\t0.0515159225484\n",
      "Valid cost:\t0.070807620883\n",
      "Epoch 95 Learning rate: 0.97337937355\n",
      "Average cost:\t0.0517200607328\n",
      "Valid cost:\t0.0713635832071\n",
      "Epoch 96 Learning rate: 0.953911781311\n",
      "Average cost:\t0.0499093112356\n",
      "Valid cost:\t0.0712871477008\n",
      "Epoch 97 Learning rate: 0.934833526611\n",
      "Average cost:\t0.0513009830472\n",
      "Valid cost:\t0.0710334479809\n",
      "Epoch 98 Learning rate: 0.916136860847\n",
      "Average cost:\t0.0504038414892\n",
      "Valid cost:\t0.0718989595771\n",
      "Epoch 99 Learning rate: 0.897814154625\n",
      "Average cost:\t0.050596961649\n",
      "Valid cost:\t0.0705237388611\n",
      "0.0684956\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    #initializer=tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        # model\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        last_A=tf.transpose(outputs_A,[1,0,2])[-1]\n",
    "        last_B=tf.transpose(outputs_B,[1,0,2])[-1]\n",
    "        concat_outputs=tf.concat(1,[last_A,last_B])\n",
    "        fully_connected = tf.contrib.layers.fully_connected(concat_outputs,num_outputs=1,activation_fn=tf.tanh,biases_initializer=initializer)\n",
    "        fully_connected=tf.reshape(fully_connected,[-1])\n",
    "        prediction=tf.sigmoid(fully_connected)\n",
    "        \n",
    "        # cost\n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        for v in tf.trainable_variables():\n",
    "            print(v.name)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "            print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(train_data.sentences_A),config.batch_size))\n",
    "            print(config.learning_rate,config.max_grad_norm,config.keep_prob,config.lr_decay,config.max_epoch,config.max_max_epoch)\n",
    "            # train\n",
    "            for epoch in range(config.max_max_epoch):\n",
    "                lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "                avg_cost=0.\n",
    "                for i in range(total_batch):\n",
    "                    start=i*config.batch_size\n",
    "                    end=(i+1)*config.batch_size\n",
    "\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                    \n",
    "                start=total_batch * config.batch_size\n",
    "                end=len(train_data.sentences_A)\n",
    "                if not start==end:\n",
    "                    next_batch_input=next_batch(start,end,train_data)\n",
    "                    _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "                            sentences_A:next_batch_input.sentences_A,\n",
    "                            sentencesA_length:next_batch_input.sentencesA_length,\n",
    "                            sentences_B:next_batch_input.sentences_B,\n",
    "                            sentencesB_length:next_batch_input.sentencesB_length,\n",
    "                            labels:next_batch_input.relatedness_scores,\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                        })\n",
    "                    avg_cost+=train_cost\n",
    "                    \n",
    "                print('Average cost:\\t{}'.format(avg_cost/total_batch))\n",
    "                \n",
    "                # validation\n",
    "                valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                    sentences_A:valid_data.sentences_A,\n",
    "                    sentencesA_length:valid_data.sentencesA_length,\n",
    "                    sentences_B:valid_data.sentences_B,\n",
    "                    sentencesB_length:valid_data.sentencesB_length,\n",
    "                    labels:np.reshape(valid_data.relatedness_scores,(-1)),\n",
    "                    embedding_placeholder:init_W,\n",
    "                    dropout_f:1.0\n",
    "                })\n",
    "                print('Valid cost:\\t{}'.format(valid_cost))\n",
    "                \n",
    "            saver.save(sess, FLAGS.save_path+'stsrnn-model',global_step=config.max_max_epoch)\n",
    "\n",
    "            # test\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "                sentences_A:test_data.sentences_A,\n",
    "                sentencesA_length:test_data.sentencesA_length,\n",
    "                sentences_B:test_data.sentences_B,\n",
    "                sentencesB_length:test_data.sentencesB_length,\n",
    "                labels:np.reshape(test_data.relatedness_scores,(-1)),\n",
    "                embedding_placeholder:init_W,\n",
    "                dropout_f:1.0\n",
    "            })\n",
    "            print(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('SICK/stsrnn_trial_result00.txt','w') as fw:\n",
    "    fw.write('pair_ID\trelatedness_score\tentailment_judgment\\n')\n",
    "    for _ in range(len(valid_predict)):\n",
    "        fw.write(valid_data.pairIDs[_]+'\\t'+str(valid_predict[_]*4+1)+'\\tNA\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.79734462 -0.79734087 -0.79735208 ..., -0.79735351 -0.79735398 -0.797355  ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        sentences_A=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_A')\n",
    "        sentencesA_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesA_length')\n",
    "        sentences_B=tf.placeholder(tf.int32,shape=([None,FLAGS.max_length]),name='sentences_B')\n",
    "        sentencesB_length=tf.placeholder(tf.int32,shape=([None]),name='sentencesB_length')\n",
    "        labels=tf.placeholder(tf.float32,shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f=tf.placeholder(tf.float32)\n",
    "        W=tf.Variable(tf.constant(0.0,shape=[len(dictionary),FLAGS.embedding_dim]),trainable=False,name='W')\n",
    "        embedding_placeholder=tf.placeholder(data_type(),[len(dictionary),FLAGS.embedding_dim])\n",
    "        embedding_init=W.assign(embedding_placeholder)\n",
    "\n",
    "        sentences_A_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "        sentences_B_emb=tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "        with tf.variable_scope('siamese') as scope:\n",
    "            outputs_A,last_states_A=build_model(sentences_A_emb,sentencesA_length,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            outputs_B,last_states_B=build_model(sentences_B_emb,sentencesB_length,dropout_f)\n",
    "\n",
    "        last_A=tf.transpose(outputs_A,[1,0,2])[-1]\n",
    "        last_B=tf.transpose(outputs_B,[1,0,2])[-1]\n",
    "        concat_outputs=tf.concat(1,[last_A,last_B])\n",
    "        fully_connected = tf.contrib.layers.fully_connected(concat_outputs,num_outputs=1,activation_fn=tf.tanh,biases_initializer=initializer)\n",
    "        fully_connected=tf.reshape(fully_connected,[-1])\n",
    "        prediction=tf.sigmoid(fully_connected)\n",
    "        \n",
    "        cost=tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr=tf.Variable(0.0,trainable=False)\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer=tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "        train_op=optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr=tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update=tf.assign(lr,new_lr)\n",
    "        \n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             saver = tf.train.Saver()\n",
    "#             total_batch=int(len(train_data.sentences_A)/config.batch_size)\n",
    "#             print('Total batch size: {}'.format(total_batch))\n",
    "            \n",
    "            fc,lb,pr,co=sess.run([fully_connected,labels,prediction,cost],feed_dict={\n",
    "                    sentences_A:train_data.sentences_A,\n",
    "                            sentencesA_length:train_data.sentencesA_length,\n",
    "                            sentences_B:train_data.sentences_B,\n",
    "                            sentencesB_length:train_data.sentencesB_length,\n",
    "                            labels:np.reshape(train_data.relatedness_scores,(-1)),\n",
    "                            dropout_f:config.keep_prob,\n",
    "                            embedding_placeholder:init_W\n",
    "                })\n",
    "            print(fc)\n",
    "#             # train\n",
    "#             for epoch in range(config.max_max_epoch):\n",
    "#                 lr_decay=config.lr_decay**max(epoch+1-config.max_epoch,0.0)\n",
    "#                 sess.run([lr,lr_update],feed_dict={new_lr:config.learning_rate*lr_decay})\n",
    "#                 print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "#                 avg_cost=0.\n",
    "#                 for i in range(total_batch):\n",
    "#                     start=i*config.batch_size\n",
    "#                     end=(i+1)*config.batch_size\n",
    "\n",
    "#                     next_batch_input=next_batch(start,end,train_data)\n",
    "#                     _,train_cost,train_predict=sess.run([train_op,cost,prediction],feed_dict={\n",
    "#                             sentences_A:next_batch_input.sentences_A,\n",
    "#                             sentencesA_length:next_batch_input.sentencesA_length,\n",
    "#                             sentences_B:next_batch_input.sentences_B,\n",
    "#                             sentencesB_length:next_batch_input.sentencesB_length,\n",
    "#                             labels:next_batch_input.relatedness_scores,\n",
    "#                             dropout_f:config.keep_prob,\n",
    "#                             embedding_placeholder:init_W\n",
    "#                         })\n",
    "#                     avg_cost+=train_cost\n",
    "                    \n",
    "#                 print('Average cost: {}'.format(avg_cost/total_batch))\n",
    "                \n",
    "#                 # validation\n",
    "#                 valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                     sentences_A:valid_data.sentences_A,\n",
    "#                     sentencesA_length:valid_data.sentencesA_length,\n",
    "#                     sentences_B:valid_data.sentences_B,\n",
    "#                     sentencesB_length:valid_data.sentencesB_length,\n",
    "#                     labels:np.reshape(valid_data.relatedness_scores,(len(valid_data.relatedness_scores),1)),\n",
    "#                     embedding_placeholder:init_W,\n",
    "#                     dropout_f:1.0\n",
    "#                 })\n",
    "#                 print('Valid cost: {}'.format(valid_cost))\n",
    "                \n",
    "#             saver.save(sess, 'SICK/STS_log/stsrnn-model', global_step=max_max_epoch)\n",
    "\n",
    "#             # test\n",
    "#             test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                 sentences_A:test_data.sentences_A,\n",
    "#                 sentencesA_length:test_data.sentencesA_length,\n",
    "#                 sentences_B:test_data.sentences_B,\n",
    "#                 sentencesB_length:test_data.sentencesB_length,\n",
    "#                 labels:np.reshape(test_data.relatedness_scores,(len(test_data.relatedness_scores),1)),\n",
    "#                 embedding_placeholder:init_W,\n",
    "#                 dropout_f:1.0\n",
    "#             })\n",
    "#             print(test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31059381,  0.31059462,  0.3105922 ,  0.31060046,  0.310601  ,\n",
       "        0.31059167,  0.31059521,  0.31059521,  0.31059155,  0.31059137,\n",
       "        0.31059182,  0.31059241,  0.31060091,  0.31059223,  0.31059167,\n",
       "        0.31059173,  0.31059158,  0.31059167,  0.31059167,  0.31059164,\n",
       "        0.31059167,  0.31059167,  0.31059167,  0.31059164,  0.31059241,\n",
       "        0.31059164,  0.31059229,  0.310592  ,  0.31059161,  0.31059265,\n",
       "        0.3105911 ,  0.31059214,  0.31059223,  0.31059217,  0.310592  ,\n",
       "        0.3105917 ,  0.31059167,  0.31059149,  0.31059161,  0.31059161,\n",
       "        0.31059155,  0.31059173,  0.31059167,  0.31059161,  0.31059167,\n",
       "        0.31059366,  0.3105917 ,  0.31059173,  0.31059158,  0.31059176], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.875     ,  0.55000001,  0.92500001, ...,  0.        ,\n",
       "        0.05      ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i, x in enumerate(pr) if x<0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56440616, -0.23940539, -0.61440778, -0.28939956, -0.36439902,\n",
       "       -0.43940833, -0.31440479, -0.2394048 , -0.13940844, -0.36440864,\n",
       "       -0.28940821, -0.66440761, -0.33939907, -0.38940775, -0.28940836,\n",
       "       -0.66440833, -0.41440845, -0.29315832, -0.36440834, -0.53940839], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pr-lb)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16785094"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
